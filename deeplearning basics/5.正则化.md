通过将模型的总复杂程度加到损失函数中，可以限制模型的复杂程度，避免过拟合。  
总体形式为：  
$$
L = L_{\text{data}} + \lambda \, \Omega(\theta)
$$

其中：  
- $L_{\text{data}}$ ：原始损失（如交叉熵、MSE）  
- $\Omega(\theta)$ ：正则项，用于衡量模型的复杂程度  
- $\lambda$ ：正则化强度（超参数）

---

## 常见的正则项

### 1. L2 正则化（Ridge, 权重衰减）
- 定义：  
  $\Omega(\theta) = \frac{1}{2}\|\theta\|_2^2 = \frac{1}{2}\sum_i \theta_i^2$
- 特点：  
  - 惩罚权重的平方  
  - 倾向于使权重整体较小、分布均匀  
  - 在深度学习中常称为 **weight decay**

### 2. L1 正则化（Lasso）
- 定义：  
  $\Omega(\theta) = \|\theta\|_1 = \sum_i |\theta_i|$
- 特点：  
  - 倾向于产生稀疏解（很多参数变为 0）  
  - 可用于特征选择

### 3. 弹性网络（Elastic Net）
- 定义：  
  $\Omega(\theta) = \alpha \|\theta\|_1 + (1-\alpha)\frac{1}{2}\|\theta\|_2^2$
- 特点：  
  - L1 与 L2 的结合  
  - 既能稀疏化，又能保持稳定性