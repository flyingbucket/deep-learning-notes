## 1. 熵（Entropy）

熵是信息论中的概念，用来度量一个随机变量的不确定性。  
设离散随机变量 $X$ 可能的取值为 ${x_1, x_2, \dots, x_n}$，概率分布为  

$$
P(X=x_i) = p_i, \quad \sum_{i=1}^n p_i = 1  
$$

熵的定义是：  
$$  
H(P) = -\sum_{i=1}^n p_i \log p_i  
$$

意义：概率分布越均匀，熵越大（不确定性高）。概率分布越集中，熵越小。

---

## 2. 交叉熵（Cross-Entropy）

交叉熵用来度量两个分布 (P) 和 (Q) 之间的差异，其中：

- $P = (p_1, p_2, \dots, p_n)$ 表示真实分布（ground truth）
    
- $Q = (q_1, q_2, \dots, q_n)$ 表示预测分布（模型输出）
    

交叉熵定义为：  
$$ 
H(P, Q) = -\sum_{i=1}^n p_i \log q_i  
$$

 在分类问题中，真实标签通常是 **one-hot 向量**，比如真实类别是第 (k) 类，则：  
$$  
H(P, Q) = - \log q_k  
$$

这就是常用的分类损失函数 **交叉熵损失（Cross-Entropy Loss）**。


## 3. Softmax

Softmax 用于将模型输出的 **实数向量** 转换为 **概率分布**。

设模型输出的 logits 向量为：  
$$
z = (z_1, z_2, \dots, z_n)  
$$

Softmax 公式为：  
$$ 
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}, \quad i=1,\dots,n  
$$

性质：

- 输出结果满足 $q_i \geq 0$，且 $\sum_i q_i = 1$。
    
- 常作为分类任务最后一层，把 logits 变成概率分布 $Q = (q_1, \dots, q_n)$。


## 4. Softmax 损失（Softmax Loss）

在分类任务里，Softmax 函数通常和 **交叉熵损失** 结合使用，这个组合有时就被称为 **Softmax 损失**。

假设有k类，对于一个样本，设真实标签为 (y)，则该样本在各类的概率分布$\{p_1, \cdots,p_k\}$并用 one-hot 向量表示：  
$$
p_i =
\left\{
\begin{array}{ll}
1, & i = y \\
0, & i \neq y
\end{array}
\right.
$$
预测分布是 $q_i = \text{softmax}(z_i)$。  
交叉熵损失是：  
$$  
L = -\sum_{i=1}^n p_i \log q_i  
$$

由于 one-hot 的性质，只有正确类别的那一项保留下来，所以：  
$$  
L = - \log q_y  
$$

再把 $q_y$ 展开：  
$$  
q_y = \frac{e^{z_y}}{\sum_{j=1}^n e^{z_j}}  
$$

最终 单个样本的Softmax 损失函数为：  
$$  
L = - \log \frac{e^{z_y}}{\sum_{j=1}^n e^{z_j}}  
= - z_y + \log \left( \sum_{j=1}^n e^{z_j} \right)  
$$

对于整个输入batch，将各个样本的Softmax损失取平均，作为该batch的总体损失。

---

## 3. 总结

- **Softmax**：把 logits 转为概率分布。
    
- **Cross-Entropy**：度量预测分布和真实分布的差异。
    
- **Softmax Loss**：就是“Softmax + 交叉熵”的组合公式：  
    $$ 
    L = - z_y + \log \left( \sum_{j=1}^n e^{z_j} \right)  
    $$
## 示例
```python
class SoftmaxClassifier:
    """
    这是一个基于softmax损失函数的线性分类器
    """

    def __init__(self, Xtr, Ytr):
        self.Xtr = Xtr
        self.Ytr = Ytr
        self.feature_dim = Xtr.shape[1]
        self.num_classes = len(torch.unique(Ytr))
        self.W = torch.randn(
	        self.feature_dim, self.num_classes, requires_grad=True
	        )
        self.b = torch.zeros(1, self.num_classes, requires_grad=True)

    @staticmethod
    def softmax(X):
        X_exp = torch.exp(X)
        partition = X_exp.sum(1, keepdim=True)
        return X_exp / partition

    def loss(self, X, Y, reg):
        num_train = X.shape[0]
        scores = X.mm(self.W) + self.b
        probs = self.softmax(scores)
        correct_logprobs = -torch.log(probs[range(num_train), Y])
        data_loss = correct_logprobs.mean()
        reg_loss = 0.5 * reg * (self.W**2).sum()
        loss = data_loss + reg_loss
        return loss

```