## 近邻算法
首先给出一组数据$X_{\mathbf{N} \times \mathbf{D}}$以及对应的分类标签$Y_\mathbf{N}$作为训练数据，数据量为$N$,特征数为$D$,类别数$m$为$Y$中各不相同的数字的个数。一般来说$N>m$。
推理时给出需要推理的D维向量$X_{0}$，计算它与所有已知标签的向量(即$X$行向量)的L1距离。
向量$X_0$与向量$X_j$的L1距离定义为：
$$
d_{L1}=\sum_{i=1}^{D} |X_0^i-X_j^i|
$$

找出其中$d_{L1}$最小的$j$,对应的$Y_j$即为$X_0$的分类标签。
代码如下：
```python
class NeraestNeibour:
	def __init__(self):
		pass
	def train(self,Xtr,Ytr):
		self.Xtr=Xtr
		self.Ytr=Ytr
	def predict(self,X):
		num_test=X.shape[0]
		Ypred=np.zeros(num_test,dtype=self.Xtr.dtype)
		for i in range(num_test):
			d_L1_vec=np.sum(np.abs(self.Xtr-X[i:]),axis=1)
			# d_L1_vec是一个N维向量，记录了X[i]到所有已知标签向量X的L1距离
			# 这行代码利用了numpy的数组广播，求和时axis=1表示沿着列号增大的方向求和
			min_index=np.argmin(d_L1_vec)
			Ypred=self.Ytr[min_index]
		return Ypred
```

## K近邻算法
K近邻算法是近邻算法的扩展，近邻算法只关心最近的一个点的标签，而K近邻则用最近的K个点的标签进行投票决定未知点的分类标签预测值。计算出`d_L1_vec`后，取出其中最小的前K名，找出前K名对应的标签，以其中占**众数**的标签为未知点的预测值。
代码如下：
```python
class KNearestNeighbor:
    def __init__(self):
        pass
    
    def train(self, Xtr, Ytr):
        self.Xtr = Xtr
        self.Ytr = Ytr
    
    def predict(self, X, k=1):
        num_test = X.shape[0]
        Ypred = np.zeros(num_test, dtype=self.Ytr.dtype)
        
        for i in range(num_test):
            # 计算测试点X[i]与所有训练点的L1距离
            d_L1_vec = np.sum(np.abs(self.Xtr - X[i]), axis=1)
            # d_L1_vec是一个N维向量，记录了X[i]到所有已知标签向量X的L1距离
            
            # 找出距离最小的k个点的索引
            min_indices = np.argsort(d_L1_vec)[:k]
            
            # 获取这k个最近邻的标签
            k_nearest_labels = self.Ytr[min_indices]
            
            # 找出出现频率最高的标签（众数）
            unique_labels, counts = np.unique(
	            k_nearest_labels,return_counts=True
	            )
            most_frequent_label = unique_labels[np.argmax(counts)]
            
            Ypred[i] = most_frequent_label
            
        return Ypred
```
**由此可见，近邻算法本质上就是K=1的K近邻算法**
## 距离的定义

上述算法中使用的L1距离在数学上称为曼哈顿距离，常用的还有欧式距离和余弦距离。
### 曼哈顿距离 (L1距离)
$$d_{L1} = \sum_{i=1}^{D} |X_0^i - X_j^i|$$

### 欧式距离 (L2距离)
$$d_{L2} = \sqrt{\sum_{i=1}^{D} (X_0^i - X_j^i)^2}$$
### 余弦距离
$$d_{\cos} = 1 - \frac{X_0 \cdot X_j}{||X_0||_2 \cdot ||X_j||_2}$$
其中：
- $X_0$ 和 $X_j$ 分别是两个D维向量
- $||X||_2 = \sqrt{\sum_{i=1}^{D} X_i^2}$ 是向量的L2范数

### 各距离的特点k：
- **曼哈顿距离**：计算路径长度，对异常值相对不敏感
- **欧式距离**：最直观的距离度量，在欧几里得空间中两点间直线距离
- **余弦距离**：关注向量的方向而非大小，适用于文本分类等场景

## 参数K的选择
上述K近邻算法并不是标准意义上的深度学习，因为它是一套固定的运算机制，只能算作一种程序算法。
在K近邻算法中，参数K原本是一个需要手动选择的超参数，但在深度学习中，我们可以考虑让模型来自动选择超参数，对超参数的自动选择是深度学习的关键。
将训练数据分为互不相交的三个部分：训练集(training)、验证集(validation)、测试集(test)

**注：严格来讲，数据的三部分不仅需要不相交，他们需要在概率意义上独立。
在机器学习和深度学习中，如果数据量较小，往往采用各种重抽样的方式对数据进行增强，将重抽样的数据进行训练集、验证集、测试集划分是错误的，因为尽管每条数据之间各不相同，但它们在概率意义上不独立。正确的做法是先划分再重抽样增强。**

算法遵循以下步骤：

- 1.我门们先给定K
- 2.在训练集上得到分类模型
- 3.在验证集上进行推理
- 4.比对推理结果与验证集标签并计算正确率
- 5.调节参数K并重复步骤2至5直到得到满意的超参数K
- 6.在测试集上推理并计算最终模型的准确率

**注意：一个常见的误区是，只将数据分为训练集和测试集，在测试集上调节参数K,最终在测试集上计算模型准确率，这种方法是错误的。测试集相当于一场考试，验证集是模拟考试，如果模拟考试和真实的考试题目一样，那么就失去了学习和考试的意义。这样得到的模型准确率是虚高的。**

